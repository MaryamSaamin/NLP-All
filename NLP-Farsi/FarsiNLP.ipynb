{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# HazmÂ ğŸ“š\n",
        "ğŸ“Â Persianâ€¯NLP toolkit\n",
        "\n",
        "ğŸ› ï¸Â Normalises script\n",
        "\n",
        "ğŸ”Â Tokenises & stems\n",
        "\n",
        "ğŸ·ï¸Â POSâ€¯tagging, lemmatisation\n",
        "\n",
        "ğŸŒÂ Basic dependency parsing"
      ],
      "metadata": {
        "id": "uPXAS8qEeLiY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIYdn1woOS1n"
      },
      "outputs": [],
      "source": [
        "!pip install hazm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from hazm import *\n"
      ],
      "metadata": {
        "id": "617Amxpi7SHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalizer = Normalizer()\n",
        "normalizer.normalize('Ù…ÙŠâ€Œ Ø±ÙˆØ¯ 123Ù¤ÛµÛ¶')\n"
      ],
      "metadata": {
        "id": "LRrytqgD7MHv",
        "outputId": "44a1d4a4-6d6f-4951-99c5-134d88a673ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ù…ÛŒ\\u200cØ±ÙˆØ¯ Û±Û²Û³Û´ÛµÛ¶'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spacy_chunker = SpacyChunker(model_path = 'model_path')\n",
        "tree = spacy_chunker.parse(sentence = [('Ù†Ø§Ù…Ù‡', 'NOUN,EZ'), ('Ø§ÛŒØ´Ø§Ù†', 'PRON'), ('Ø±Ø§', 'ADP'), ('Ø¯Ø±ÛŒØ§ÙØª', 'NOUN'), ('Ø¯Ø§Ø´ØªÙ…', 'VERB'), ('.', 'PUNCT')])\n",
        "print(tree)\n"
      ],
      "metadata": {
        "id": "4met4Bzg7W4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from hazm import word_tokenize\n",
        "\n",
        "sent = \"Ú©ØªØ§Ø¨Ù… Ø±Ø§ Ø¨Ù‡ Ø§Ùˆ Ù…ÛŒâ€ŒØ¯Ù‡Ù….\"\n",
        "word_tokenize(sent)"
      ],
      "metadata": {
        "id": "DAIF6ilc7rys",
        "outputId": "d25165c7-942f-4e78-da8a-97775967e697",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Ú©ØªØ§Ø¨Ù…', 'Ø±Ø§', 'Ø¨Ù‡', 'Ø§Ùˆ', 'Ù…ÛŒ\\u200cØ¯Ù‡Ù…', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stanza"
      ],
      "metadata": {
        "id": "iu5velmoPFMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from stanza import Pipeline\n",
        "nlp = Pipeline('fa', processors='tokenize,pos,lemma')\n",
        "doc = nlp(\"Ú©ØªØ§Ø¨â€ŒÙ‡Ø§ÛŒÙ…Ø§Ù† Ø±Ø§ Ù…ÛŒâ€ŒØ¢ÙˆØ±Ù†Ø¯\")\n",
        "[ (w.text, w.lemma) for w in doc.iter_words() ]"
      ],
      "metadata": {
        "id": "-h2AnKP_OqP1",
        "outputId": "06f3d571-0c59-44e5-a89f-b5eb7a4b8d46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Ú©ØªØ§Ø¨\\u200cÙ‡Ø§ÛŒ', 'Ú©ØªØ§Ø¨'),\n",
              " ('Ù…Ø§Ù†', 'Ù…Ø§'),\n",
              " ('Ø±Ø§', 'Ø±Ø§'),\n",
              " ('Ù…ÛŒ\\u200cØ¢ÙˆØ±Ù†Ø¯', 'Ø¢ÙˆØ±Ø¯')]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stanzaâ€‘faÂ ğŸ§ \n",
        "âš¡Â GPUâ€‘readyÂ UDâ€¯pipeline\n",
        "\n",
        "ğŸ”Â TokenizeÂ +Â multiâ€‘wordâ€¯split\n",
        "\n",
        "ğŸ·ï¸Â POS, lemma, morphology\n",
        "\n",
        "ğŸŒ³Â BiLSTMâ€¯dependencyâ€¯parser\n",
        "\n",
        "ğŸ†”Â NER finetuned onÂ PEYMA"
      ],
      "metadata": {
        "id": "6n9IWv-geQkp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stanza\n"
      ],
      "metadata": {
        "id": "JYY4IzMA64uP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import stanza, spacy\n",
        "# Stanza pipeline with GPU\n",
        "nlp = stanza.Pipeline('fa', processors='tokenize,mwt,pos,lemma,depparse', use_gpu=True)\n",
        "doc = nlp(\"Ø¯ÛŒØ±ÙˆØ² Ú©ØªØ§Ø¨ Ø¬Ø°Ø§Ø¨ÛŒ Ø¨Ø±Ø§ÛŒ Ø¹Ù„ÛŒ Ø®Ø±ÛŒØ¯Ù….\")\n",
        "\n",
        "# spaCy visual (RTL)\n",
        "nlp_sp = spacy.blank(\"fa_udpipe\")\n",
        "# nlp_sp.add_pipe(\"dependency_parser\")\n",
        "doc.sentences[0].print_dependencies()\n",
        "spacy.displacy.render(nlp_sp(doc.text), style=\"dep\", options={'direction':'rtl'})"
      ],
      "metadata": {
        "id": "vLKwH-vR6p7J",
        "outputId": "784ac0ec-a31c-4d1e-d960-fb8a921af453",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Ø¯ÛŒØ±ÙˆØ²', 6, 'obl')\n",
            "('Ú©ØªØ§Ø¨', 6, 'obj')\n",
            "('Ø¬Ø°Ø§Ø¨ÛŒ', 2, 'amod')\n",
            "('Ø¨Ø±Ø§ÛŒ', 5, 'case')\n",
            "('Ø¹Ù„ÛŒ', 6, 'obl')\n",
            "('Ø®Ø±ÛŒØ¯Ù…', 0, 'root')\n",
            "('.', 6, 'punct')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"fa\" id=\"7570b8df02fa43c0988d3df3dc758de1-0\" class=\"displacy\" width=\"1100\" height=\"137.0\" direction=\"rtl\" style=\"max-width: none; height: 137.0px; color: #000000; background: #ffffff; font-family: Arial; direction: rtl\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"47.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1050\">Ø¯ÛŒØ±ÙˆØ²</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1050\"></tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"47.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"875\">Ú©ØªØ§Ø¨</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"875\"></tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"47.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"700\">Ø¬Ø°Ø§Ø¨ÛŒ</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"700\"></tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"47.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"525\">Ø¨Ø±Ø§ÛŒ</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"525\"></tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"47.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"350\">Ø¹Ù„ÛŒ</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"350\"></tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"47.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"175\">Ø®Ø±ÛŒØ¯Ù….</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"175\"></tspan>\n",
              "</text>\n",
              "</svg></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ParsivarÂ ğŸ“š\n",
        "* Ruleâ€‘based Persian preâ€‘processor\n",
        "\n",
        "* Normalization, tokenization, stemming\n",
        "\n",
        "* POSâ€¯tagger and lemmatizer\n",
        "\n",
        "* NER with CRF model\n",
        "\n",
        "* Lightweight, pureâ€‘Python friendly"
      ],
      "metadata": {
        "id": "JBs5O1NYeTJn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "81f66fc1-6eb0-41c9-cd95-de59f4a35c6c",
        "id": "AcztcUsjeBzs"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting parsivar\n",
            "  Downloading parsivar-0.2.3.1-py3-none-any.whl.metadata (242 bytes)\n",
            "Requirement already satisfied: nltk>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from parsivar) (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.6.6->parsivar) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.6.6->parsivar) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.6.6->parsivar) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.6.6->parsivar) (4.67.1)\n",
            "Downloading parsivar-0.2.3.1-py3-none-any.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m104.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: parsivar\n",
            "Successfully installed parsivar-0.2.3.1\n"
          ]
        }
      ],
      "source": [
        "!pip install parsivar\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tmp_text = \" 123 Û±Û²Û³ Ø§ØµÙ„Ø§Ø­ Ù†ÙˆÙŠØ³Ù‡ Ù‡Ø§ Ùˆ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù†ÛŒÙ…â€ŒÙØ§ØµÙ„Ù‡ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø±Ø§ Ø¢Ø³Ø§Ù† Ù…ÙŠ ÙƒÙ†Ø¯\"\n",
        "\n",
        "from parsivar import Normalizer\n",
        "my_normalizer = Normalizer()\n",
        "print(my_normalizer.normalize(tmp_text))\n"
      ],
      "metadata": {
        "id": "390ocbacOEN5",
        "outputId": "725d5cac-ee36-456b-bb0c-551d3d60fe61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "123 123 Ø§ØµÙ„Ø§Ø­ Ù†ÙˆÛŒØ³Ù‡â€ŒÙ‡Ø§ Ùˆ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù†ÛŒÙ…â€ŒÙØ§ØµÙ„Ù‡ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø±Ø§ Ø¢Ø³Ø§Ù† Ù…ÛŒâ€ŒÚ©Ù†Ø¯\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_normalizer = Normalizer(statistical_space_correction=True)\n",
        "print(my_normalizer.normalize(tmp_text))\n"
      ],
      "metadata": {
        "id": "bqBqK6rPOe5-",
        "outputId": "956958cf-455d-4446-9d7f-ec99fba3fe0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "123 123 Ø§ØµÙ„Ø§Ø­â€ŒÙ†ÙˆÛŒØ³Ù‡â€ŒÙ‡Ø§ Ùˆ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù†ÛŒÙ…â€ŒÙØ§ØµÙ„Ù‡ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø±Ø§ Ø¢Ø³Ø§Ù† Ù…ÛŒâ€ŒÚ©Ù†Ø¯\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_normalizer = Normalizer(date_normalizing_needed=True)\n",
        "print(my_normalizer.normalize(tmp_text))\n"
      ],
      "metadata": {
        "id": "EG2ysO67OpYv",
        "outputId": "88f9dd90-2b22-4e07-a65e-0e7ae528a719",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15129 Ø§ØµÙ„Ø§Ø­ Ù†ÙˆÛŒØ³Ù‡â€ŒÙ‡Ø§ Ùˆ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù†ÛŒÙ…â€ŒÙØ§ØµÙ„Ù‡ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø±Ø§ Ø¢Ø³Ø§Ù† Ù…ÛŒâ€ŒÚ©Ù†Ø¯\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_normalizer = Normalizer(pinglish_conversion_needed=True)\n",
        "print(my_normalizer.normalize(\"farda asman abri ast.\"))\n"
      ],
      "metadata": {
        "id": "UNaZZ5tZO0mF",
        "outputId": "053c8095-9a35-4dd1-cea4-701f10f14eaf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ÙØ±Ø¯Ø§ Ø§Ø³Ù…Ø§Ù† Ø§Ø¨Ø±ÛŒ Ø§Ø³Øª .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from parsivar import FindStems\n",
        "my_stemmer = FindStems()\n",
        "print(my_stemmer.convert_to_stem(\"Ø¨ÛŒØ§Ø¨ÛŒÙ…\"))\n"
      ],
      "metadata": {
        "id": "b3bwjw9GO4wF",
        "outputId": "0a364607-0c5e-4cf9-be24-8fb5518eeca5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ÛŒØ§ÙØª&ÛŒØ§Ø¨\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERTâ€‘faÂ ğŸ¤–\n",
        "* Transformer Persian language backbone\n",
        "\n",
        "* Pretrained on 3â€¯Bâ€‘token corpus\n",
        "\n",
        "* Embeddings for NER, sentiment\n",
        "\n",
        "* HFâ€¯id: bert-fa-base\n",
        "\n",
        "* Fineâ€‘tune, deploy with ease"
      ],
      "metadata": {
        "id": "o2sdGbFdeWZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers\n"
      ],
      "metadata": {
        "id": "6XbrdMDtDa8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "model_id = \"HooshvareLab/bert-fa-base-uncased-ner-peyma\"   # PEYMAâ€‘only\n",
        "# model_id = \"HooshvareLab/bert-fa-zwnj-base-ner\"          # mixed ARMAN+PEYMA+WikiANN\n",
        "ner = pipeline(\"ner\", model=model_id, aggregation_strategy=\"simple\")\n",
        "\n",
        "text = \"Ø¯Ú©ØªØ± Ø³Ø§Ø±Ø§ Ú¯Ù„â€ŒÙ…Ø­Ù…Ø¯ÛŒ Ø¯Ø± ØªÙ‡Ø±Ø§Ù† Ø³Ø®Ù†Ø±Ø§Ù†ÛŒ Ú©Ø±Ø¯.\"\n",
        "print(ner(text))\n"
      ],
      "metadata": {
        "id": "IVzBueI49Q8d",
        "outputId": "7ccc0e16-1b4a-45fd-d4a6-7bea5c284fa5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'entity_group': 'B_PER', 'score': np.float32(0.999181), 'word': 'Ø³Ø§Ø±Ø§', 'start': 5, 'end': 9}, {'entity_group': 'I_PER', 'score': np.float32(0.9997402), 'word': 'Ú¯Ù„Ù…Ø­Ù…Ø¯ÛŒ', 'start': 10, 'end': 18}, {'entity_group': 'B_LOC', 'score': np.float32(0.99893755), 'word': 'ØªÙ‡Ø±Ø§Ù†', 'start': 22, 'end': 27}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "sent = pipeline('sentiment-analysis',\n",
        "                model='HooshvareLab/bert-fa-base-uncased-sentiment-digikala')\n",
        "\n",
        "print(sent(\"Ø§ÛŒÙ† Ú¯ÙˆØ´ÛŒ ÙˆØ§Ù‚Ø¹Ø§Ù‹ Ø¨ÛŒâ€ŒÙ†Ø¸ÛŒØ±Ù‡!\"))"
      ],
      "metadata": {
        "id": "sSgSyC4vGEFB",
        "outputId": "a25dbbe9-99f7-45c3-8e1c-c105cf6b324a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'recommended', 'score': 0.9822326898574829}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT2â€‘faÂ âœï¸\n",
        "* Autoregressive Persian language model\n",
        "\n",
        "* 124â€¯M parameters, GPTâ€‘2 architecture\n",
        "\n",
        "* Trained on 500â€¯M Persian tokens\n",
        "\n",
        "* Text generation, continuation, summarisation\n",
        "\n",
        "* HF id: HooshvareLab/gpt2-fa\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GqR9HjL-e14K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Editable prompt and generation settings\n",
        "system_instruction = \"Ø´Ù…Ø§ ÛŒÚ© Ø¯Ø³ØªÛŒØ§Ø± ÙØ§Ø±Ø³ÛŒ Ù‡Ø³ØªÛŒØ¯ Ú©Ù‡ Ø¨Ù‡ Ø³Ø¤Ø§Ù„Ø§Øª Ø¨Ø§ Ù„Ø­Ù† Ø¯ÙˆØ³ØªØ§Ù†Ù‡ Ù¾Ø§Ø³Ø® Ù…ÛŒâ€ŒØ¯Ù‡ÛŒØ¯.\"\n",
        "user_prompt = \"Ø³Ù„Ø§Ù…! Ù…ÛŒØ´Ù‡ Ø¯Ø±Ø¨Ø§Ø±Ù‡Ù” Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ ØªÙˆØ¶ÛŒØ­ Ø¨Ø¯ÛŒØŸ\"\n",
        "full_prompt = system_instruction + \"\\n\\n\" + user_prompt\n",
        "\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Load pre-trained Farsi text generation model\n",
        "model_name = \"HooshvareLab/gpt2-fa\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Encode input and generate output\n",
        "inputs = tokenizer(full_prompt, return_tensors=\"pt\")\n",
        "outputs = model.generate(**inputs, max_new_tokens=500, do_sample=True)\n",
        "\n",
        "# Decode and remove prompt from output\n",
        "decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "generated_text = decoded[len(full_prompt):].strip()\n",
        "\n",
        "print(\"ğŸŸ¢ Ù¾Ø§Ø³Ø®:\\n\", generated_text)\n"
      ],
      "metadata": {
        "id": "qyaotEwSe3Vf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}